---
title: "similarity analyses of mystery data"
author: "Jesper Bruun"
date: "3/31/2020"
output: github_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```
## Import data set
```{r load-data, echo=T}
mydata<-read.csv("data/data_spring2019.csv",sep=";")
mydata
```
## Load necesseary functions
```{r functions, echo=T}
library(igraph)
source("functions/backboneExtraction.r")
source("functions/segregation.r")
```

### Student similarity network

```{r make-networks, echo=T}

  #Function for calculating frequencies (here equated with probabilities) of responses
probs<-function(mydata,n){
  a<-as.numeric(names(table(mydata[,n])))
  p<-as.numeric(table(mydata[,n],useNA = "always")/length(mydata[,1]))
  x<-c(1:length(mydata[,1]))
  for(i in 1:length(a)){
    x[which(mydata[,n]==a[i])]<-p[i]
  }
  x[which(is.na(mydata[,n]))]<-p[6]
  return(x)
}

#Function which transforms frequencies/probabilities to information (bits)
pmat<-function(data){
pmat<-matrix(0,ncol=length(data),nrow=length(data[,1]))
for(j in 1:length(data)){
  pmat[,j]<-probs(data,j)  
  
}
infmat<--log2(pmat)
return(infmat)
}

  #Function for calculating similarities between respondents
  #The function uses Lin's (1998) information theoretical measure. 
simRes<-function(i,j,infmat,d){
  y<-infmat[i,]
  overlap<-sum(y[which(d[i,1:24]==d[j,1:24])])
  sinfi<-sum(infmat[i,1:24])
  sinfj<-sum(infmat[j,1:24])
  sim<-2*overlap/(sinfi+sinfj)
  return(sim)
}

  #Function for calculating similarity between k'th respondent and everyone else
simResk<-function(k,inf,d){
    simVec<-vector()
  for(i in 1:length(d[,1])){
    simVec[i]<-simRes(k,i,inf,d)
  }
  return(simVec)
}

  #Function for making similarity matrix. Really just a for-loop that uses simResk 
simMatrix<-function(d){
  inf<-pmat(d)
  similarityMatrix<-matrix(data=0,ncol=length(d[,1]),nrow=length(d[,1]))
  for(i in 1:length(d[,1])){
    similarityMatrix[,i]<-simResk(i,inf,d)  
    
  }
  return(similarityMatrix)
}


```
Now we use igraph to make the network
```{r make-student-network}
M<-simMatrix(mydata)
StudNet<-graph.adjacency(M,diag=F,weighted=T)
studid<-paste("S",c(1:length(mydata[,1])),sep="")
V(StudNet)$id<-studid
```
As expected, the network is close to fully connected, $L=4804$, $L_{fc}=70*69=4830$. We use LANS (Foti et al 2011) to extract a backbone network. The algorithm evalueates the links emanating from each node and selects the ones above a certain threshold (below a certain significance level). As a rule of thumb, we might say that we want a connected network. So, we run the algorithm multiple times to find the significance level (that is each node keeps only links that are significant to the chosen level), which leaves us with a connected network. 
```{r plot-student-network}

StudNetbb05<-backboneNetwork(StudNet,0.05,2)
plot(StudNetbb05)

StudNetbb03<-backboneNetwork(StudNet,0.03,2)
plot(StudNetbb03)

StudNetbb02<-backboneNetwork(StudNet,0.02,2)
plot(StudNetbb02)

StudNetbb015<-backboneNetwork(StudNet,0.015,2)
plot(StudNetbb015)

StudNetbb015<-backboneNetwork(StudNet,0.014,2)
plot(StudNetbb015)


```

Now run community detection. I have been using infomap for a long time, but the implementation in igraph is maybe not the best. We would probably want a managable amount of communities with at enough people in each group to do some kind of statistical analysis. 
```{r community-detection}
IM<-infomap.community(StudNetbb015)
IM
table(IM$membership)
FG<-fastgreedy.community(StudNetbb015)
FG
table(IM$membership)
```
Since FG and IM give the same modularity (Q) and FG has fewer modules, for this example, we keep that. Now, we can analyse the answers of each group. Here is a start:
```{r response-analysis}
mydata[FG$membership==1,]
mydata[FG$membership==2,]
mydata[FG$membership==3,]
mydata[FG$membership==4,]
mydata[FG$membership==5,]
mydata[FG$membership==6,]
mydata[FG$membership==7,]
```
By analysing these tables, we can see, which answers seem to bind these groups together. This will vary from group to group. This first analysis could be furhter refined in many ways -- for example, one could find a criterion for selecting particular questions that are more suited for making clusters. 


